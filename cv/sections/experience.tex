\documentclass[letter,10pt]{article}
\usepackage{TLCresume}
\begin{document}

%====================
% EXPERIENCE A
%====================
\subsection{{Data Architect \hfill Jun 2021 --- Present}}
\subsection{{Data Engineer \hfill Dec 2018 --- Jun 2021}}
\subtext{Solactive \hfill Frankfurt}

\subsubsection*{{Key Responsibilities}}

\begin{zitemize}



	\item Designing multiple micro-service based end-to-end data pipelines using Kafka-stream and pythons Faust Framework.
	
	\item Onboarding several vendors for market data using various delivery methods and data structures. Reaching from on-demand APIs to complex formats delivered via file protocols

	\item Building a data warehouse for complex-data including data streams and feeds from multiple vendors including end-of-day as well as intraday market data. 

	\item Developing of a cryptocurrency-calculation-engine for indices and ETFs based on CoinMarketCap's API.
	
	\item Establishing a Log-workflow with standards log utilities, EFK-Stack and Grafana
	
	\item Implementing a Full-Stack Dashboard to monitor data-pipelines and view process statuses for business teams via python and dash
	
	\item Providing several guidelines and documentations to point the developers in a direct of a proper micro-service (12-Factor-App) inspired design and promote decoupling, cohesion and SOLID-principles to increase the quality and lower the cost of development. 
	
	
	\item  Composing guidelines for local testing (Unit \& Integration Test), system testing and User-Acceptance-Test(UAT) and their integration in the service life-cycle.
	
	\item Creating several utilities and deployment templates to implement the new Configuration, Test and Release Management. Including building several CI/CD pipelines as well as the creation of several helpers (based on cookiecutter) that create boilerplate code and folder structures. 
	
	\item Designing, implementing and enforcing of several ITIL conform standards regarding configuration (CM), test (TM) and release (RM) management.
	
	
	\item Increasing process transparency via openTelemetry, Jaeger and Grafana 
	
	\item Establishing and maintaining standards for software, code and review quality 
	
	
		
	\item Training of business-user, analysts and developers in python and general programming
	
	\item Hiring, supervising  and  training  of  the  departments  software-engineering interns and new joiners
\end{zitemize}

\subsubsection*{{Technologies}}

Python, Kafka, Clickhouse, MySQL, MongoDB, Docker, RabbitMQ, Jenkins, Redis, Airflow, Rundeck, Linux, Grafana, EFK-Stack \vspace{1em}

%====================
% EXPERIENCE B
%====================

\subsection{{Consultant Data Scientist \hfill Oct 2017 --- Nov 2018}}
\subtext{STATWORX \hfill Frankfurt}


\subsubsection*{{Key Responsibilities}}

\begin{zitemize}
\item Support and Manage of major projects by creating architectures, ML models, data pipelines and presentations. \item Holding a biweekly python workshop to train fellow employees.  
\item Numerous side projects \begin{itemize}
	\item development of a AI based Tool to estimate the the success rate for invites on business portal like Linked-In or Xing.
	\item support of a development to classify and cost estimation of damages on building based on drone images
	\item consulting clients by providing concepts of database-systems, data-pipelines and integration of data science into their product and project life-cycle.s
\end{itemize}
\end{zitemize}

\subsubsection*{{Technologies}}
R, Python,Docker, PostgreSQL, Linux, Google Cloud

\vspace{1em}

\subsection{{Project: Multinational retailer  \hfill Oct.  2017 --- Nov. 2018 }}
\subtext{STATWORX \hfill Frankfurt}
\subsubsection*{{Outline}}
Creation of a web based Application to evaluate and forecast customers effect on changes in the pricing strategy. The underlying simulation supported either models either based on statistical elasticity or on a random forest. 
\subsubsection*{{Key Responsibilities}}
\begin{zitemize} 
	\item Extending and fine-tuning the statistical and machine learning models by enriching the underlying business model and data pipelines. 
	\item Designing the technical architecture to bring the POC into production 
	\item Automating of the data preparation via R, python and airflow and deploy it via docker on premises. 
	\item Design and implementing the Full-Stack application using R, R-Shiny and JavaScript.     
	\item Deploying the Tech-Stack on multiple platform over its lifetime including on-prem, Azure and Google Cloud.
	\item Holding trainings, workshops and presentations for the clients to handle the application from the business, technical and management perspective. 

\end{zitemize}


\subsubsection*{{Technologies}}
R, Python, Airflow, Docker, PostgreSQL, Linux, Azure, Google Cloud
\end{document}